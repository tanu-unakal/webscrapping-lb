{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://dblp.org/db/conf/sc/index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "important = soup.select('.publ-list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confererence  2006\n",
      "sending request to https://dblp.org/db/conf/sc/sc2006.html\n",
      "getting abstract from https://doi.org/10.1145/1188455.1188459\n",
      "<Response [403]>\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-04f87fe4e818>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0msoup3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"article__section article__abstract hlFld-Abstract\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mabstracts_tanu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./abstracts/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"foryear_\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "abstracts_tanu = []\n",
    "#for each year in the conference\n",
    "i = 2006\n",
    "\n",
    "for div in soup.find_all(class_='publ-list')[13:]: #remove 0 and do it for all\n",
    "    print('confererence ', i)\n",
    "    childdiv = div.find_all(class_='entry editor toc')[0]\n",
    "    dataINchild = childdiv.find_all(class_='data')[0]\n",
    "    nextfind = dataINchild.find_all(class_=\"toc-link\")[0]['href']\n",
    "    print('sending request to',nextfind)\n",
    "    res2 = requests.get(nextfind)\n",
    "    soup2 = BeautifulSoup(res2.text, 'html.parser') #res not res2?\n",
    "    important2 = soup.select('.publ-list')\n",
    "    #for the first one in that year, all the papers in that MAIN contents\n",
    "    j = 2\n",
    "    for div in soup2.find_all(class_=\"entry inproceedings\")[3:]: #do this for all of them\n",
    "        #sleep(120)\n",
    "        child2 = div.find_all(class_= 'head')[0]\n",
    "        abstractAT = child2.a['href']\n",
    "        print('getting abstract from', abstractAT)\n",
    "        \n",
    "        res3 = requests.get(abstractAT)\n",
    "        print(res3)\n",
    "        soup3 = BeautifulSoup(res3.text, 'html.parser')\n",
    "        a1 = soup3.find_all(class_=\"article__section article__abstract hlFld-Abstract\")[0].p\n",
    "        abstracts_tanu.append(a1.get_text())\n",
    "        with open(\"./abstracts/\"+str(j)+\"foryear_\"+ str(i)+ \".txt\", \"w+\") as f:\n",
    "            f.write(a1.get_text())\n",
    "        print('got it!')\n",
    "        \n",
    "        j +=1\n",
    "        \n",
    "        \n",
    "    i -=1\n",
    "    print('done w an entire year!!!')\n",
    "print('done w an entire conference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting abstract from https://dl.acm.org/doi/10.1145/1188455.1188458\n",
      "<Response [200]>\n",
      "<p>The Dynamic Data Driven Applications Systems (DDDAS) concept entails capabilities where application simulations can dynamically accept and respond to field-data and measurements, and/or can control such measurements. This synergistic and symbiotic feedback control-loop between simulations and measurements goes beyond the traditional control systems approaches, and advances applications and measurement approaches, beneficially impacting science and engineering fields, as well as manufacturing, commerce, transportation, hazard prediction/management, medicine, etc. DDDAS environments extend the current computational grids. The multi-agency DDDAS Program Solicitation (www.cise.nsf.gov/dddas) fosters systematically the relevant research areas. NSF, NOAA and NIH, the NSF/OISE and SBIR Offices, and the EU-IST and e-Sciences Programs are cooperating sponsors. This session will consist of a panel of experts, including awardees of DDDAS projects and representatives from funding agencies, and will provide a forum to engage the broader community in open discussion for expanding the opportunities and impact of DDDAS.</p>\n"
     ]
    }
   ],
   "source": [
    "abstractAT = 'https://dl.acm.org/doi/10.1145/1188455.1188458'\n",
    "print('getting abstract from', abstractAT)\n",
    "        \n",
    "res3 = requests.get(abstractAT)\n",
    "print(res3)\n",
    "soup3 = BeautifulSoup(res3.text, 'html.parser')\n",
    "a1 = soup3.find_all(class_=\"article__section article__abstract hlFld-Abstract\")[0].p\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'abstracts_tanu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cb796fb9c347>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstracts_tanu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'abstracts_tanu' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(abstracts_tanu))\n",
    "not got - 3, 5, 7, 2012,2010, 2008,2007, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confererence  0\n",
      "sending request to  https://dblp.org/db/conf/sc/sc2019.html\n",
      "refernce\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for div in soup.find_all(class_='publ-list')[:1]: #remove 0 and do it for all\n",
    "    print('confererence ', i)\n",
    "    childdiv = div.find_all(class_='entry editor toc')[0]\n",
    "    dataINchild = childdiv.find_all(class_='data')[0]\n",
    "    nextfind = dataINchild.find_all(class_=\"toc-link\")[0]['href']\n",
    "    print('sending request to ',nextfind)\n",
    "    res2 = requests.get(nextfind)\n",
    "    soup2 = BeautifulSoup(res2.text, 'html.parser') #res not res2?\n",
    "    important2 = soup.select('.publ-list')\n",
    "    #for the first one in that year, all the papers in that MAIN contents\n",
    "    for div in soup2.find_all(class_=\"entry inproceedings\")[:5]:\n",
    "        child2 = div.find_all(class_= 'head')[0]\n",
    "        abstractAT = child2.a['href']\n",
    "        \n",
    "        #print(abstractAT)\n",
    "        #print('/n')\n",
    "        #print('next')\n",
    "        #print('getting abstract from', abstractAT)\n",
    "\n",
    "        res3 = requests.get(abstractAT)\n",
    "        soup3 = BeautifulSoup(res3.text, 'html.parser')\n",
    "        a1 = soup3.find_all(class_=\"article__section article__abstract hlFld-Abstract\")[0].p\n",
    "            #abstracts_tanu.append(a1.get_text())\n",
    "        #print(a1.get_text())\n",
    "print('refernce')\n",
    "tannu = requests.get('https://doi.org/10.1145/3295500')\n",
    "soup3 = BeautifulSoup(tannu.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
